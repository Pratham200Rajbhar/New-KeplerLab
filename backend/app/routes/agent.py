"""Agent routes — code execution and research, all routed through LangGraph agent.

Provides:
- POST /agent/execute — Direct Python code execution (user-authored code, SSE stream)
- POST /agent/analyze — NL → code → execute, via LangGraph DATA_ANALYSIS intent (SSE stream)
- POST /agent/research — Deep research, via LangGraph RESEARCH intent (SSE stream)
- GET  /agent/status/{job_id} — Execution status

All endpoints share the same single execution engine: the LangGraph agent graph.
/execute is the only exception — it accepts user-written code directly for the code REPL,
bypassing NL understanding but still running through the same sandbox.
"""

import json
import logging
import uuid
import time
from fastapi import APIRouter, Depends
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from typing import Optional, List

from app.services.agent.persistence import log_code_execution
from app.services.auth import get_current_user

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/agent", tags=["agent"])

_SSE_HEADERS = {
    "Cache-Control": "no-cache",
    "Connection": "keep-alive",
    "X-Accel-Buffering": "no",
}


class ExecuteRequest(BaseModel):
    """Direct Python code execution (code-editor / REPL use-case)."""
    code: str
    notebook_id: str
    timeout: int = 15


class AnalyzeRequest(BaseModel):
    """NL data-analysis request.  material_ids must be uploaded CSV materials."""
    query: str
    notebook_id: str
    material_ids: Optional[List[str]] = None


class ResearchRequest(BaseModel):
    """Deep web-research request."""
    query: str
    notebook_id: str
    material_ids: Optional[List[str]] = None


# ── /execute — direct sandbox (user writes the code) ─────────

@router.post("/execute")
async def execute_code_endpoint(
    request: ExecuteRequest,
    current_user=Depends(get_current_user),
):
    """Execute user-authored Python code directly in the sandbox with SSE streaming.

    This is the code-REPL path.  The code is not generated by the LLM; it is
    provided verbatim by the user.  Security validation still applies.

    SSE events:  stdout → result/error → done
    """
    from app.services.code_execution.executor import execute_code
    import asyncio

    session_id = str(uuid.uuid4())
    queue: asyncio.Queue = asyncio.Queue()

    async def on_stdout_line(line: str):
        await queue.put({"type": "stdout", "line": line})

    async def execute_task():
        try:
            result = await execute_code(
                code=request.code,
                timeout=min(request.timeout, 15),
                on_stdout_line=on_stdout_line,
            )
            await log_code_execution(
                user_id=str(current_user.id),
                notebook_id=request.notebook_id,
                code=request.code,
                stdout=result.get("stdout", ""),
                stderr=result.get("stderr", ""),
                exit_code=result.get("exit_code", -1),
                has_chart=result.get("chart_base64") is not None,
                elapsed=result.get("elapsed", 0.0),
            )
            await queue.put({"type": "result", "data": result})
        except Exception as e:
            logger.error("Execute task failed: %s", e)
            await queue.put({"type": "error", "error": str(e)})

    task = asyncio.create_task(execute_task())

    async def event_generator():
        try:
            yield f"event: start\ndata: {json.dumps({'session_id': session_id})}\n\n"
            while True:
                msg = await queue.get()
                if msg["type"] == "stdout":
                    yield f"event: stdout\ndata: {json.dumps({'session_id': session_id, 'line': msg['line']})}\n\n"
                elif msg["type"] == "result":
                    yield f"event: result\ndata: {json.dumps(msg['data'])}\n\n"
                    break
                elif msg["type"] == "error":
                    yield f"event: error\ndata: {json.dumps({'session_id': session_id, 'error': msg['error']})}\n\n"
                    break
        except Exception as e:
            logger.error("event_generator failed: %s", e)
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"
        finally:
            # Always emit done and cancel hanging task
            yield f"event: done\ndata: {json.dumps({'session_id': session_id})}\n\n"
            if not task.done():
                task.cancel()

    return StreamingResponse(event_generator(), media_type="text/event-stream", headers=_SSE_HEADERS)


# ── /analyze — routes through LangGraph agent (DATA_ANALYSIS) ─

@router.post("/analyze")
async def analyze_data_endpoint(
    request: AnalyzeRequest,
    current_user=Depends(get_current_user),
):
    """NL data-analysis — routed through LangGraph agent with DATA_ANALYSIS intent.

    Pre-sets intent and plan so the agent skips redundant detection/planning.
    material_ids should point to uploaded CSV materials.

    SSE events: start → step → token → code_stdout → meta → done
    """
    from app.services.agent.graph import run_agent_stream
    from app.services.agent.state import AgentState

    session_id = str(uuid.uuid4())

    initial_state: AgentState = {
        "user_message": request.query,
        "user_id": str(current_user.id),
        "notebook_id": request.notebook_id,
        "material_ids": request.material_ids or [],
        "session_id": session_id,
        # Pre-set intent so intent_detection + planner are bypassed
        "intent": "DATA_ANALYSIS",
        "intent_confidence": 1.0,
        "requires_planning": False,
        "plan": [{"tool": "python_tool", "description": "Analyze data and generate chart"}],
        "current_step": 0,
        "tool_results": [],
        "iterations": 0,
        "total_tokens": 0,
        "total_tool_calls": 0,
        "needs_retry": False,
        "step_retries": 0,
        "response": "",
        "agent_metadata": {},
    }

    async def generate():
        try:
            async for event in run_agent_stream(initial_state):
                yield event
        except Exception as e:
            logger.error("analyze stream failed: %s", e)
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"
            yield f"event: done\ndata: {json.dumps({'session_id': session_id})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream", headers=_SSE_HEADERS)


# ── /research — routes through LangGraph agent (RESEARCH) ────

@router.post("/research")
async def research_endpoint(
    request: ResearchRequest,
    current_user=Depends(get_current_user),
):
    """Deep research — routed through LangGraph agent with RESEARCH intent.

    Pre-sets intent and plan so the agent skips redundant detection/planning.

    SSE events: start → step → token → meta → done
    """
    from app.services.agent.graph import run_agent_stream
    from app.services.agent.state import AgentState

    session_id = str(uuid.uuid4())

    initial_state: AgentState = {
        "user_message": request.query,
        "user_id": str(current_user.id),
        "notebook_id": request.notebook_id,
        "material_ids": request.material_ids or [],
        "session_id": session_id,
        # Pre-set intent so intent_detection + planner are bypassed
        "intent": "RESEARCH",
        "intent_confidence": 1.0,
        "requires_planning": False,
        "plan": [{"tool": "research_tool", "description": "Conduct deep research on the topic"}],
        "current_step": 0,
        "tool_results": [],
        "iterations": 0,
        "total_tokens": 0,
        "total_tool_calls": 0,
        "needs_retry": False,
        "step_retries": 0,
        "response": "",
        "agent_metadata": {},
    }

    async def generate():
        try:
            async for event in run_agent_stream(initial_state):
                yield event
        except Exception as e:
            logger.error("research stream failed: %s", e)
            yield f"event: error\ndata: {json.dumps({'error': str(e)})}\n\n"
            yield f"event: done\ndata: {json.dumps({'session_id': session_id})}\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream", headers=_SSE_HEADERS)


# ── /status — execution status placeholder ────────────────────

@router.get("/status/{job_id}")
async def execution_status(
    job_id: str,
    current_user=Depends(get_current_user),
):
    """Execution status — synchronous executions return immediately."""
    return JSONResponse(content={
        "job_id": job_id,
        "status": "completed",
        "message": "Synchronous execution — result was returned immediately",
    })
