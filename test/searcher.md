---

## 1. Search API
Perform a search exploration on Google or Bing. 

**Engines:** `duckduckgo`, or `all`

```bash
curl -X POST http://159.89.166.91:8002/api/search \
     -H "Content-Type: application/json" \
     -d '{
           "query": "llm finetuning ieee research paper filetype:pdf",
           "engine": "duckduckgo"
         }'
```
Output : {"engine":"duckduckgo","organic_results":[{"position":1,"title":"PDFEmpirical Study of LLM Fine-Tuning for Text Classification in Legal ...","link":"https://edrm.net/wp-content/uploads/2023/12/ieee-2023-empirical-study-of-llm-finetuning-for-text-classification-in-legal-document-review.pdf","snippet":"Thispaperpresents a series of experiments comparing a standard, pretrained DistilBERT model and a fine-tuned DistilBERT model, both leveraged for the downstream NLP task of text classification. Tuning the model using domain-specific data from real-world legal matters suggestsfine-tuningimproves the performance ofLLMtext classifiers.","displayed_link":"edrm.net"},{"position":2,"title":"PDFMobiLLM: Enabling On-Device Fine-Tuning of Billion-Sized LLMs via ...","link":"https://uwaterloo.ca/scholar/sites/ca.scholar/files/sshen/files/li2025mobillm.pdf","snippet":"However, it faces signiﬁcant challenges due to prohibitive memory requirements and slow training speeds. In thispaper, we propose MobiLLM, a novel scheme enabling memory-efﬁcient LLMﬁne-tuningonasinglemobiledevice viaserver-assistedside- tuning.","displayed_link":"uwaterloo.ca"},{"position":3,"title":"JOURNAL OF LA SplitLoRA: A Split Parameter-Efficient Fine-Tuning ...","link":"https://arxiv.org/pdf/2407.00952","snippet":"To fill this gap, in thispaper, we propose a concise andresearch-friendly SLLLMfine-tuningframework, named SplitLoRA. SplitLoRA is built on the split federated learning (SFL) [30] framework that amalgamates the advantages of parallel training from FL and model splitting from SL and is developed on the well-known parameter-efficientfine-tuning(PEFT)","displayed_link":"arxiv.org"},{"position":4,"title":"PDFFramework for Data Generation and LLM Finetuning Evaluation","link":"https://www.ltimindtree.com/wp-content/uploads/2025/04/Framework-for-Data-Generation-and-LLM-Finetuning.pdf","snippet":"Abstract Thispaperpresents the results of LTIMindtree's comprehensive study onfine-tuningLarge Language Models (LLMs) to develop an advanced Artificial Intelligence (AI) chatbot, specifically tailored to provide information on AI policies, laws, and regulations. To showcase the efficiency of using a largerLLMfor creating labeled data from raw text,","displayed_link":"ltimindtree.com"},{"position":5,"title":"PDFThe impact of LLM pruning for fine-tuning - Stanford University","link":"https://web.stanford.edu/class/cs224n/final-reports/256911740.pdf","snippet":"Abstract Scaling laws in the deep learning era have inspired the scaling of large language models (LLMs) to parameter sizes that require extraordinary amounts of compute and energy for both training and inference. Such large pretrained models, or foundation models, provide valuable model initializations for subsequentfine-tuning. Recent work has co","displayed_link":"web.stanford.edu"},{"position":6,"title":"PDFZeroth-Order Fine-Tuning of LLMs in Random Subspaces","link":"https://openaccess.thecvf.com/content/ICCV2025/papers/Yu_Zeroth-Order_Fine-Tuning_of_LLMs_in_Random_Subspaces_ICCV_2025_paper.pdf","snippet":"To enhance memory efficiency, MeZO [40] first introduces the zeroth-order (ZO) optimizer toLLMfine-tuningwithout BP. It only requires forward passes and calculates gradi-ent estimates using finite differences of training loss values, enabling it to directly handle non-differentiable objectives.","displayed_link":"openaccess.thecvf.com"},{"position":7,"title":"Understanding the Performance and Estimating the Cost of LLM Fine-Tuning","link":"https://arxiv.org/pdf/2408.04693","snippet":"2Georgia Institute of Technology Abstract—Due to the cost-prohibitive nature of training Large Language Models (LLMs),fine-tuninghas emerged as an attrac-tive alternative for specializingLLMsfor specific tasks using limited compute resources in a cost-effective manner. In thispaper, we characterize sparse Mixture of Experts (MoE) basedLLMfine-tunin","displayed_link":"arxiv.org"},{"position":8,"title":"PDFLLM Finetuning Techniques","link":"https://mlsyscourse.org/slides/15-LLM-finetuning.pdf","snippet":"Adapter Tuning Add new adapter modules (with a few parameters) to anLLMDuringfinetuning, freeze the original network and only finetune adapters","displayed_link":"mlsyscourse.org"},{"position":9,"title":"PDFSplit Fine-Tuning for Large Language Models in Wireless Networks","link":"https://uwaterloo.ca/scholar/sites/ca.scholar/files/sshen/files/zhang2025split.pdf","snippet":"In thispaper, we proposeanefﬁcientLLMﬁne-tuningschemeinwirelessnetworks, named SplitFine-Tuning(SFT), which can accommodateLLMﬁne-tuningon mobile devices. Speciﬁcally, anLLMis split into a server-side part on the edge server and a device-side part on the mobile device to satisfy the device-side memory constraint.","displayed_link":"uwaterloo.ca"},{"position":10,"title":"Fine Tuning Llm For Sentiment Analysis - nano-ntp.com","link":"https://nano-ntp.com/index.php/nano/article/download/5447/4360/10667","snippet":"May 20, 2025Thispaperprovides the methodology forfine-tuningLLMs, including solutions to issues such as dataset imbalances, overfitting, and the selection of the correct pre-trained models. By empirical findings, thepaperillustrates how fine-tunedLLMscan enhance sentiment analysis accuracy and stability, providing insights for practical application","displayed_link":"nano-ntp.com"}],"related_questions":[],"knowledge_graph":null}%                                                               
---

## 2. Scrape API
Extract clean text content and metadata from any public URL.

```bash
curl -X POST http://159.89.166.91:8002/api/scrape \
     -H "Content-Type: application/json" \
     -d '{
           "url": "https://example.com"
         }'
```
Ouput :
{"content":{"url":"https://example.com","title":"Example Domain","content":["This domain is for use in documentation examples without needing permission. Avoid use in operations."],"meta_description":"","word_count":15,"extracted_at":"2026-02-23T15:09:13.381317"}}%                                            
---